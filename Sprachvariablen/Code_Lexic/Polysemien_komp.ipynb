{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "625bcd26",
   "metadata": {},
   "source": [
    "_syn_, _lnsyn_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69120279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.lemmatize.lat import LatinBackoffLemmatizer\n",
    "from cltk.wordnet.wordnet import WordNetCorpusReader\n",
    "from cltk.core.exceptions import CLTKException\n",
    "import string\n",
    "import re\n",
    "import numpy\n",
    "from numpy import log as ln\n",
    "\n",
    "def polysemy(sentence):\n",
    "    # Create WordNet Corpus Reader for Latin\n",
    "    LWN = WordNetCorpusReader(iso_code=\"lat\")\n",
    "\n",
    "    # Remove punctuation marks from sentence\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Split sentence into words\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Lemmatize the words\n",
    "    lemmatizer = LatinBackoffLemmatizer()\n",
    "    lemmas = lemmatizer.lemmatize(words)\n",
    "\n",
    "    # The LatinBackoffLemmatizer gives us a tuple of the token and the lemma.\n",
    "    # Because we only need the lemma, we extract the lemma from the tuple.\n",
    "    # We also remove numerals 1 to 4 that may appear after a lemma.\n",
    "    lemmata = [re.sub(r'[1-4]$', '', lemma[1]) for lemma in lemmas]\n",
    "\n",
    "    total_synonyms = 0\n",
    "    logfreq_sum = 0\n",
    "\n",
    "    for lemma in lemmata:\n",
    "        if lemma:\n",
    "            try:\n",
    "                # Get the synsets for the lemma\n",
    "                lemma_synsets = LWN.lemma(lemma)\n",
    "\n",
    "                if lemma_synsets:\n",
    "                    synsets = list(lemma_synsets[0].synsets())\n",
    "                    num_synonyms = len(synsets)\n",
    "                    total_synonyms += num_synonyms\n",
    "\n",
    "                    # Calculate the logarithm of the number of synonyms (only when num_synonyms >= 1)\n",
    "                    if num_synonyms >= 1:\n",
    "                        log_num_synonyms = ln(num_synonyms)\n",
    "                        logfreq_sum += log_num_synonyms\n",
    "\n",
    "                else:\n",
    "                    num_synonyms = 0\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    \n",
    "    avg_synonyms = round(total_synonyms / len(words),3) if len(words) > 0 else 0\n",
    "    log_synonyms = round(logfreq_sum / len(words),3) if len(words) > 0 else 0\n",
    "\n",
    "    return avg_synonyms, log_synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef92c22",
   "metadata": {},
   "source": [
    "_synsw_, _lnsynsw_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "844acebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.lemmatize.lat import LatinBackoffLemmatizer\n",
    "from cltk.wordnet.wordnet import WordNetCorpusReader\n",
    "from cltk.stops.words import Stops\n",
    "from cltk.core.exceptions import CLTKException\n",
    "import string\n",
    "import re\n",
    "import numpy\n",
    "from numpy import log as ln\n",
    "\n",
    "def polysemy_cnt(sentence):\n",
    "    # Create WordNet Corpus Reader for Latin\n",
    "    LWN = WordNetCorpusReader(iso_code=\"lat\")\n",
    "\n",
    "    # Remove punctuation marks from sentence\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Split sentence into words\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Lemmatize the words\n",
    "    lemmatizer = LatinBackoffLemmatizer()\n",
    "    lemmas = lemmatizer.lemmatize(words)\n",
    "\n",
    "    # The LatinBackoffLemmatizer gives us a tuple of the token and the lemma.\n",
    "    # Because we only need the lemma, we extract the lemma from the tuple.\n",
    "    # We also remove numerals 1 to 4 that may appear after a lemma.\n",
    "    lemmata = [re.sub(r'[1-4]$', '', lemma[1]) for lemma in lemmas]\n",
    "    \n",
    "    # we remove the Latin stop words.\n",
    "    stops_obj = Stops(iso_code=\"lat\")\n",
    "    tokens_filtered = stops_obj.remove_stopwords(tokens=lemmata)\n",
    "\n",
    "    total_synonyms = 0\n",
    "    logfreq_sum = 0\n",
    "\n",
    "    for lemma in tokens_filtered:\n",
    "        if lemma:\n",
    "            try:\n",
    "                # Get the synsets for the lemma\n",
    "                lemma_synsets = LWN.lemma(lemma)\n",
    "\n",
    "                if lemma_synsets:\n",
    "                    synsets = list(lemma_synsets[0].synsets())\n",
    "                    num_synonyms = len(synsets)\n",
    "                    total_synonyms += num_synonyms\n",
    "\n",
    "                    # Calculate the logarithm of the number of synonyms (only when num_synonyms >= 1)\n",
    "                    if num_synonyms >= 1:\n",
    "                        log_num_synonyms = ln(num_synonyms)\n",
    "                        logfreq_sum += log_num_synonyms\n",
    "\n",
    "                else:\n",
    "                    num_synonyms = 0\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    \n",
    "    avg_synonyms = round(total_synonyms / len(words),3) if len(words) > 0 else 0\n",
    "    log_synonyms = round(logfreq_sum / len(words),3) if len(words) > 0 else 0\n",
    "\n",
    "    return avg_synonyms, log_synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e7a566",
   "metadata": {},
   "source": [
    "_syn500_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f384e438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.lemmatize.lat import LatinBackoffLemmatizer\n",
    "from cltk.wordnet.wordnet import WordNetCorpusReader\n",
    "from cltk.stops.words import Stops\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def polysemy_counter(sentence):\n",
    "    \n",
    "    # Remove punctuation marks from sentence\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Split sentence into words\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Lemmatize the words\n",
    "    lemmatizer = LatinBackoffLemmatizer()\n",
    "    lemmas = lemmatizer.lemmatize(words)\n",
    "\n",
    "    # The LatinBackoffLemmatizer gives us a tuple of the token and the lemma.\n",
    "    # Because we only need the lemma, we extract the lemma from the tuple.\n",
    "    # We also remove numerals 1 to 4 that may appear after a lemma.\n",
    "    lemmata = [re.sub(r'[1-4]$', '', lemma[1]) for lemma in lemmas]\n",
    "    \n",
    "    # we remove the Latin stop words.\n",
    "    stops_obj = Stops(iso_code=\"lat\")\n",
    "    tokens_filtered = stops_obj.remove_stopwords(tokens=lemmata)\n",
    "\n",
    "    # Read the Excel file into a DataFrame\n",
    "    df = pd.read_excel('Polysemien_OLD.xlsx')\n",
    "\n",
    "    # Create a dictionary from the Excel data\n",
    "    polysemy_dict = dict(zip(df['LEMMA'], df['POLYSEM']))\n",
    "    \n",
    "    # Count the polysems\n",
    "    polysem_count = 0\n",
    "    for lemma in tokens_filtered:\n",
    "        if lemma in polysemy_dict:\n",
    "            polysem_count += polysemy_dict[lemma]\n",
    "        else:\n",
    "            polysem_count += 1\n",
    "    \n",
    "    avg_synonyms = round(polysem_count / len(words),3)\n",
    "\n",
    "    return avg_synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34b294d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21.937, 1.425)\n"
     ]
    }
   ],
   "source": [
    "sentence = \"maxime Teucrorum ductor, quo sospite numquam res equidem Troiae victas aut regna fatebor, nobis ad belli auxilium pro nomine tanto exiguae vires; hinc Tusco claudimur amni, hinc Rutulus premit et murum circumsonat armis. sed tibi ego ingentis populos opulentaque regnis iungere castra paro, quam fors inopina salutem ostentat: fatis huc te poscentibus adfers. haud procul hinc saxo incolitur fundata vetusto urbis Agyllinae sedes, ubi Lydia quondam gens, bello praeclara, iugis insedit Etruscis. hanc multos florentem annos rex deinde superbo imperio et saevis tenuit Mezentius armis. quid memorem infandas caedes, quid facta tyranni effera? di capiti ipsius generique reservent! mortua quin etiam iungebat corpora vivis componens manibusque manus atque oribus ora, tormenti genus, et sanie taboque fluentis complexu in misero longa sic morte necabat. at fessi tandem cives infanda furentem armati circumsistunt ipsumque domumque, obtruncant socios, ignem ad fastigia iactant. ille inter caedem Rutulorum elapsus in agros confugere et Turni defendier hospitis armis. ergo omnis furiis surrexit Etruria iustis, regem ad supplicium praesenti Marte reposcunt. his ego te, Aenea, ductorem milibus addam. toto namque fremunt condensae litore puppes signaque ferre iubent, retinet longaevus haruspex fata canens: o Maeoniae delecta iuventus, flos veterum virtusque virum, quos iustus in hostem fert dolor et merita accendit Mezentius ira, nulli fas Italo tantam subiungere gentem: externos optate duces. tum Etrusca resedit hoc acies campo monitis exterrita divum.\"\n",
    "print(polysemy(sentence))\n",
    "print(polysemy_cnt(sentence))\n",
    "print(polysemy_counter(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e747825",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
